{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for the model ##\n",
    "\n",
    "The processed data used in the paper is available on the github repo. However, detailed information on steps required to process the data is not. These pre-processing notebooks aim to create a dataset that can be easily used by the model.\n",
    "\n",
    "We take the following steps:\n",
    "\n",
    "<img src=\"../../images/data_path.png\" alt=\"Data Path\" style=\"width:60%;\">\n",
    "\n",
    "Demean, detrend, and resample are done in the [Data_Cleaning](Data_Cleaning.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read the files and segment them into one hour windows. Each file will contain 28,800 samples per hour at an 8 Hz sampling rate. The data is saved in `data/segmented`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import read\n",
    "import glob\n",
    "import numpy as np\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# Define the processed and output folder paths\n",
    "processed_folder = os.getcwd() + '/data/processed'\n",
    "segmented_output_folder = os.getcwd() + '/data/segmented'\n",
    "os.makedirs(segmented_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "# Define the target segment duration (1 hour) in seconds\n",
    "segment_duration = 3600  # seconds\n",
    "\n",
    "# Process each processed file for segmentation\n",
    "for file_path in glob.glob(f\"{processed_folder}/*.mseed\"):\n",
    "    # Read the processed file\n",
    "    st = read(file_path)\n",
    "    \n",
    "    # Split each trace in the Stream object into one-hour segments\n",
    "    for tr in st:\n",
    "        start_time = tr.stats.starttime\n",
    "        end_time = tr.stats.endtime\n",
    "        segment_start = start_time\n",
    "        \n",
    "        # Loop over each one-hour segment\n",
    "        while segment_start + segment_duration <= end_time:\n",
    "            # Define the end time for the current segment\n",
    "            segment_end = segment_start + segment_duration\n",
    "\n",
    "            # Slice the trace to create a one-hour segment\n",
    "            segment = tr.slice(starttime=segment_start, endtime=segment_end)\n",
    "            \n",
    "            # Format the filename for the segment\n",
    "            segment_filename = f\"{tr.stats.network}_{tr.stats.station}_{tr.stats.channel}_{segment_start.strftime('%Y%m%dT%H%M%S')}.mseed\"\n",
    "            segment_filepath = os.path.join(segmented_output_folder, segment_filename)\n",
    "            \n",
    "            # Save the one-hour segment as a new file\n",
    "            try:\n",
    "                segment.write(segment_filepath, format=\"MSEED\")\n",
    "                print(f\"Saved segment file: {segment_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing segment {segment_filename}: {e}\")\n",
    "\n",
    "            # Move to the next hour\n",
    "            segment_start = segment_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all files in the segmented folder for nan data\n",
    "\n",
    "for file_path in glob.glob(f\"{segmented_output_folder}/*.mseed\"):\n",
    "    st = read(file_path)\n",
    "    for tr in st:\n",
    "        if np.isnan(tr.data).any():\n",
    "            print(f\"File {file_path} contains NaN data\")\n",
    "        # else:\n",
    "        #     print(f\"File {file_path} contains no NaN data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Fourier Transform with the `specified window_length` and `hop_length parameters` to produce a spectrogram of size (96, 128) for each one-hour segment. The data is saved in `data\\transformed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "import librosa\n",
    "from scipy.signal import resample\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Define directories\n",
    "transformed_output_folder = os.getcwd() + '/data/transformed'\n",
    "os.makedirs(transformed_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "# Define constants\n",
    "target_sampling_rate = 8  # Hz\n",
    "segment_duration = 3600  # 1 hour in seconds\n",
    "window_length = 256\n",
    "hop_length = 224\n",
    "target_shape = (96, 128)\n",
    "\n",
    "# Initialize an empty list to store spectrograms\n",
    "spectrogram_list = []\n",
    "\n",
    "# Step 1: Process each file in the input folder\n",
    "for file_path in glob.glob(f\"{segmented_output_folder}/*.mseed\"):\n",
    "    # Read the file\n",
    "    st = read(file_path)\n",
    "    #st.plot()\n",
    "    \n",
    "    # Process each trace\n",
    "    for tr in st:\n",
    "        #tr.plot()\n",
    "       \n",
    "        # Split into one-hour segments (28,800 samples each)\n",
    "        start_time = tr.stats.starttime\n",
    "        end_time = tr.stats.endtime\n",
    "        segment_start = start_time\n",
    "\n",
    "        while segment_start + segment_duration <= end_time:\n",
    "            # Define the end time for the current segment\n",
    "            segment_end = segment_start + segment_duration\n",
    "\n",
    "            # Slice the trace to create a one-hour segment\n",
    "            segment = tr.slice(starttime=segment_start, endtime=segment_end)\n",
    "\n",
    "            # min max values in the segment\n",
    "            #print(f\"Min: {np.min(segment.data)}, Max: {np.max(segment.data)}\")\n",
    "\n",
    "            # Convert to PyTorch tensor\n",
    "            segment_tensor = torch.tensor(segment, dtype=torch.float32)\n",
    "\n",
    "            print(f\"Min: {torch.min(segment_tensor)}, Max: {torch.max(segment_tensor)}\")\n",
    "            \n",
    "            # Ensure correct dimensions (STFT expects 1D tensor or batch x time)\n",
    "            if len(segment_tensor.shape) == 1:\n",
    "                segment_tensor = segment_tensor.unsqueeze(0)\n",
    "                \n",
    "            # Compute the STFT\n",
    "            stft_result = torch.stft(\n",
    "                segment_tensor,\n",
    "                n_fft=window_length,\n",
    "                hop_length=hop_length,\n",
    "                win_length=window_length,\n",
    "                return_complex=True,\n",
    "            )\n",
    "                \n",
    "            # Compute the spectrogram (magnitude squared)\n",
    "            spectrogram = torch.abs(stft_result) ** 2\n",
    "\n",
    "            # Convert to NumPy for further processing\n",
    "            spectrogram = spectrogram.numpy()\n",
    "\n",
    "            # print(\"converted\")\n",
    "            # print(f'Min: {np.min(spectrogram)}, Max: {np.max(spectrogram)}')\n",
    "            \n",
    "            # Resize to a consistent shape if needed\n",
    "            target_shape = (96, 128)  # Desired shape\n",
    "            spectrogram_resized = np.resize(spectrogram, target_shape)\n",
    "            # print(\"resize\")\n",
    "            # print(f'Min: {np.min(spectrogram_resized)}, Max: {np.max(spectrogram_resized)}')\n",
    "\n",
    "            # Clip spectrogram values before log transformation NEWNEW\n",
    "            #spectrogram_resized = np.clip(spectrogram_resized, a_min=0, a_max=1e6)  # Adjust `a_max` as needed\n",
    "            # print(\" not clipped\")\n",
    "            #print(f'Min: {np.min(spectrogram_resized)}, Max: {np.max(spectrogram_resized)}')\n",
    "\n",
    "            # Log-transform the spectrogram\n",
    "            spectrogram_log = np.log1p(spectrogram_resized)\n",
    "            # print(\"log transformed\")\n",
    "            # print(f'Min: {np.min(spectrogram_log)}, Max: {np.max(spectrogram_log)}')\n",
    "\n",
    "            # Normalize between -1 and 1\n",
    "            #spectrogram_log = 2 * (spectrogram_log - np.min(spectrogram_log)) / np.ptp(spectrogram_log) - 1\n",
    "            \n",
    "            #segment.plot()\n",
    "\n",
    "            # Save the spectrogram (you could save the result as needed)\n",
    "            segment_filename = f\"{tr.stats.network}_{tr.stats.station}_{tr.stats.channel}_{segment_start.strftime('%Y%m%dT%H%M%S')}.npy\"\n",
    "            segment_filepath = os.path.join(transformed_output_folder, segment_filename)\n",
    "\n",
    "            # print(f'Min: {np.min(spectrogram_log)}, Max: {np.max(spectrogram_log)}\\n')\n",
    "\n",
    "            np.save(segment_filepath, spectrogram_log)\n",
    "\n",
    "            # Append the spectrogram to the list\n",
    "            spectrogram_list.append(spectrogram_log)\n",
    "            \n",
    "            # check the file for data outside the bounds of -1 and 1\n",
    "            # if np.any(spectrogram_log > 1) or np.any(spectrogram_log < -1):\n",
    "            #     print(f\"File {segment_filename} contains data outside the bounds of -1 and 1\")\n",
    "\n",
    "            # Move to the next hour\n",
    "            segment_start = segment_end\n",
    "            \n",
    "            # display file being processed\n",
    "            # time.sleep(0.1)\n",
    "            # clear_output(wait=True)\n",
    "            print(f'Processing file: {segment_filename}')\n",
    "            \n",
    "            #check file for nan data\n",
    "            if np.isnan(spectrogram_log).any():\n",
    "                print(f\"File {segment_filename} contains NaN data\")             \n",
    "\n",
    "# Stack all spectrograms into a single numpy array and add batch and channel dimensions\n",
    "all_spectrograms = np.stack(spectrogram_list)\n",
    "all_spectrograms = all_spectrograms[:, np.newaxis, :, :]  # Shape: (batch_size, 1, 96, 128)\n",
    "print(f\"Shape of combined spectrogram array: {all_spectrograms.shape}\")\n",
    "\n",
    "# remove the single feature dimension (1)\n",
    "all_spectrograms = np.squeeze(all_spectrograms)\n",
    "\n",
    "# NOTE: FILE NAME IS HARD CODED HERE\n",
    "# Save the combined spectrogram array to Input.npy\n",
    "final_output_folder = os.getcwd()\n",
    "input_filepath = os.path.join(final_output_folder, \"Input.npy\")\n",
    "#input_filepath = os.path.join(final_output_folder + 'NUPH_analysis', \"Input_nuph.npy\")\n",
    "np.save(input_filepath, all_spectrograms)\n",
    "\n",
    "print(f\"Saved combined spectrogram file: {input_filepath.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information on Input.npy\n",
    "print(f\"Shape of combined spectrogram array with batch and channel dimensions: {all_spectrograms.shape}\")\n",
    "print(f\"Number of files processed: {all_spectrograms.shape[0]}\")\n",
    "\n",
    "# load Input.npy and display information for confirmation\n",
    "input_data_file_t = np.load(input_filepath)\n",
    "print(f\"Shape of combined spectrogram array Input.npy: {input_data_file_t.shape}\")\n",
    "print(f\"Number of files processed: {input_data_file_t.shape[0]}\")\n",
    "\n",
    "\n",
    "# display max and min values of the Input.npy file\n",
    "print(f\"Max value in Input.npy: {np.max(input_data_file_t)}\")\n",
    "print(f\"Min value in Input.npy: {np.min(input_data_file_t)}\")\n",
    "\n",
    "\n",
    "# the numbers should match"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Input shape is [all_spectrograms.shape[0], 1, 96, 128] where:\n",
    "\n",
    "all_spectrograms.shape[0]: batch size -> the number of hours of data processed\n",
    "1: input channel. Probably indicates there is only one feature per time step\n",
    "96: height (or time)\n",
    "128: width (or frequency)  \n",
    "\n",
    "11/14/24 - input channel removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five rows in the all_spectograms\n",
    "print(all_spectrograms[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
